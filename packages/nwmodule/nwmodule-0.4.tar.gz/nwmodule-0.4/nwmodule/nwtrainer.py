import torch as tr
from pathlib import Path
from typing import Dict, Any, Iterable, Iterator, Optional
from datetime import datetime
from nwutils.others import RunningMean
from nwutils.torch import trGetData, trToDevice, npGetData, getOptimizerStr
from .nwmodule import NWModule
from .logger import logger, drange
from .metrics import Metric

# Generic module that takes a NWModule and trains it on some train/validation iterator
# TODO: Would be nice if we could work with a nn.Module directly in future and ignore some missing features
class NWTrainer:
	# @param[in] model The NWModule that is required for the training.
	def __init__(self, model:NWModule, workingDirectory:Path=None):
		self.model = model
		if workingDirectory is None:
			workingDirectory = Path.cwd()
		self.workingDirectory = workingDirectory
		self.workingDirectory.mkdir(exist_ok=True, parents=True)

	def initializeEpochMetrics(self):
		metrics = self.model.getMetrics()
		res = {name:RunningMean(initValue=metric.defaultValue()) for name, metric in metrics.items()}
		return res

	# Other neural network architectures can update these
	def callbacksOnEpochStart(self, isTraining:bool):
		# Call onEpochStart here, using only basic args
		for callback in self.model.getCallbacks():
			callback.onEpochStart(model=self.model, workingDirectory=self.workingDirectory, isTraining=isTraining)

	def epochPrologue(self, epochResults, isTraining:bool):
		res = {}
		metrics = self.model.getMetrics()
		# First, turn metrics from running means to actual numbers
		for Key in epochResults.keys():
			res[Key] = {}
			for epochKey, epochValue in epochResults[Key].items():
				if epochKey in metrics:
					epochKey = epochKey.name
					epochValue = metrics[epochKey].epochReduceFunction(epochValue.get())
				else:
					epochValue = str(epochValue)
				res[Key][epochKey] = epochValue

		if isTraining:
			optimizerStr = getOptimizerStr(self.model.getOptimizer())
			res["Optimizer"] = optimizerStr
			self.model.getTrainHistory().append(res)

		# Then, in topological sort order, call on epoch end for all callbacks.
		for key in self.model.topologicalKeys:
			assert key in self.model.callbacks
			callback = self.model.callbacks[key]
			callback.onEpochEnd(model=self.model, epochResults=res, \
				workingDirectory=self.workingDirectory, isTraining=isTraining)
		return res

	def callbacksOnIterationStart(self, isTraining:bool, isOptimizing:bool):
		for callback in self.model.getCallbacks():
			callback.onIterationStart(isTraining=isTraining, isOptimizing=isOptimizing)

	def callbacksOnIterationEnd(self, data, labels, results, loss, iteration, numIterations, \
		metricResults, isTraining, isOptimizing):
		iterResults = {}
		modelMetrics = self.model.getMetrics().values()
		# iterResults is updated at each step in the order of topological sort
		for topologicalKey in self.model.topologicalKeys:
			callback = self.model.callbacks[topologicalKey]
			callbackResult = callback.onIterationEnd(results, labels, data=data, loss=loss, \
				iteration=iteration, numIterations=numIterations, iterResults=iterResults, \
				metricResults=metricResults, isTraining=isTraining, isOptimizing=isOptimizing)
			callbackResult = callback.iterationReduceFunction(callbackResult)
			iterResults[callback] = callbackResult

			# Add it to running mean only if it's numeric. Here's why the metrics differ for different batch size
			#  values. There's no way for us to infer the batch of each iteration, so we assume it's 1.
			if callback in modelMetrics:
				assert callback.getName() in metricResults, f"Metric {callback.getName()} not in metric results"
				metricResults[callback.getName()].update(callbackResult, count=1)
		return metricResults

	# Basic method that does a forward phase for one epoch given a iterator. It can apply a step of optimizer or not
	# @param[in] Iterator Object used to get a batch of data and labels at each step
	# @param[in] stepsPerEpoch How many items to be generated by the iterator
	# @param[in] isTraining Flag to announce to callbacks/metrics if this epoch is an training or evaluationg one
	# @param[in] isTraining Flag to announce to callbacks/metrics if this epoch is an optimizing one
	# @param[in] prefix The prefix for the tqdm range and the key of the resulting dictionary
	# @return A dictionary with the metrics over all the epoch steps and the duration
	def runOneEpoch(self, iterator:Iterator, stepsPerEpoch:int, isTraining:bool, \
		isOptimizing:bool, prefix:str="") -> Dict[str, float]:
		assert stepsPerEpoch > 0
		if isOptimizing == False and tr.is_grad_enabled():
			logger.info("Warning! Not optimizing, but grad is enabled.")
		# This variable holds all the returned items of this epoch. We'll store metrics (by name) and the duration it
		#  took for the epoch to complete.
		res = self.initializeEpochMetrics()

		startTime = datetime.now()
		Range = drange(stepsPerEpoch, desc=f"[{prefix}] Iteration", postfix={str(k):0.000 for k in res})
		for i in Range:
			npItems = next(iterator)
			# Required for multi-GPU setup.
			items = trToDevice(trGetData(npItems), self.model.getDevice())
			# TODO: we still assume inputs, labels = data. Update this to be generic!
			x, gt = items["data"], items["labels"]

			self.callbacksOnIterationStart(isTraining, isOptimizing)
			y, loss = self.model.networkAlgorithm(x, gt, isTraining, isOptimizing)
			self.callbacksOnIterationEnd(x, gt, y, loss, i, stepsPerEpoch, res, isTraining, isOptimizing)

			# For performance reasons we update the shown loss less often.
			if isinstance(Range, drange) and (i % 30 == 0 or i == stepsPerEpoch - 1):
				# TODO: some architectures (i.e. graph) have deep results (so dict[dict[running_mean]])
				currentResults = {}
				for k in res:
					if not isinstance(res[k], RunningMean):
						continue

					currentResults[str(k)] = f"{res[k].get():2.3f}"
				Range.set_postfix(currentResults)

		res["duration"] = datetime.now() - startTime
		return res

	def train(self, reader:Iterable, numEpochs:int, validationReader:Optional[Iterable]=None, \
		numSteps:Optional[int]=None, validationNumSteps:Optional[int]=None):
		currentEpoch = len(self.model.getTrainHistory()) + 1
		assert currentEpoch <= numEpochs + 1, f"{currentEpoch} vs {numEpochs}"

		N = numEpochs - currentEpoch + 1
		if N == 0:
			logger.info(f"Model is already trained for {currentEpoch} epochs. Skipping.")
			return

		numSteps = numSteps if not numSteps is None else len(reader)
		validationNumSteps = None if validationReader is None else validationNumSteps if not \
			validationNumSteps is None else len(validationReader)
		logger.debug(f"Training for {N} epochs starting from epoch {currentEpoch}.")
		logger.debug(f"Train set steps per epoch: {numSteps}")
		if not validationReader is None:
			logger.debug(f"Using validation set. Num steps per epoch: {validationNumSteps}")

		self.model.numEpochs = numEpochs
		self.model.finished = False
		for i in drange(N, initial=currentEpoch, total=numEpochs, position=0, desc="Epoch"):
			if self.model.finished:
				logger.info("Finished flag was set from somewhere else. Stopping!")
				break

			self.callbacksOnEpochStart(isTraining=True)
			epochResults = {}
			# Run for training data and append the results
			# No iteration callbacks are used if there is a validation set (so iteration callbacks are only
			#  done on validation set). If no validation set is used, the iteration callbacks are used on train set.
			res = self.runOneEpoch(iter(reader), numSteps, isTraining=True, isOptimizing=True, prefix="Train")
			epochResults["Train"] = res

			# Run for validation data and append the results
			if not validationReader is None:
				prevState = self.model.train if self.model.training else self.model.eval
				self.model.eval()
				self.callbacksOnEpochStart(isTraining=True)
				with tr.no_grad():
					res = self.runOneEpoch(iter(validationReader), validationNumSteps, \
						isTraining=True, isOptimizing=False, prefix="Validation")
				epochResults["Validation"] = res
				prevState()

			self.epochPrologue(epochResults=epochResults, isTraining=True)
			if not self.model.optimizerScheduler is None:
				self.model.optimizerScheduler.step()

	def test(self, reader, numSteps:Optional[int]=None):
		numSteps = numSteps if not numSteps is None else len(reader)
		with tr.no_grad():
			self.callbacksOnEpochStart(isTraining=False)
			metricResults, duration = \
				self.runOneEpoch(iter(reader), numSteps, isTraining=False, isOptimizing=False, prefix="Test")
			res = self.epochPrologue(res, isTraining=False)
		res = {"Test" : res}
		return res
