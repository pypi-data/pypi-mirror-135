# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01_dataset.ipynb (unless otherwise specified).

__all__ = ['config', 'cfg_db', 'user', 'password', 'host', 'port', 'mydb', 'get_connection', 'get_data',
           'query_sessions', 'get_timestamps', 'get_unique_timestamps', 'print_info', 'parse_channel',
           'biometric_to_df', 'muse_channels', 'nexus_channels', 'muse_sampling_rate', 'nexus_sampling_rate',
           'query_eeg', 'eeg_to_df', 'df_to_mne', 'dpath', 'fetch_eeg', 'get_eeg', 'ppg_names', 'get_ppg',
           'get_ppg_timestamp', 'parse_ppg', 'ppg_to_df', 'get_telemetry', 'get_task', 'print_task_info',
           'sync_nback_task', 'get_user_tasks']

# Cell
import pandas as pd
import numpy as np
import time
import os
import psycopg2
import warnings
import time
from functools import partial
import mne
from pathlib import Path

# Cell
from configparser import ConfigParser
config = ConfigParser(delimiters=['='])

##Local Development
config.read('../settings.ini')

# Cell
cfg_db = config['DATABASE']
user = cfg_db['USER'].strip('\'')
password = cfg_db['PASSWORD'].strip('\'')
host = cfg_db['HOST_DEV'].strip('\'')
port = cfg_db['PORT'].strip('\'')
mydb = cfg_db['DB'].strip('\'')

# Cell
def get_connection():
    return psycopg2.connect(user=user, password=password, host=host, port=port, database=mydb)

# Cell
def get_data(sql):
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute(sql)
    data = cursor.fetchall()
    conn.close()
    return data

# Cell
def query_sessions(user_id):
    sql = f"""
    SELECT "metricId","stimulationId","Device"."type" AS "device","Session"."type","userId","Session"."createdAt","startTime","stopTime"
    FROM "Session"
    JOIN "Device" ON "Device".id = "deviceId"
    WHERE "userId" = '{user_id}'
    ORDER by "createdAt" ASC
    """

    data = get_data(sql)
    df = pd.DataFrame(data, columns=["metric_id", "stimulation_id", "device", "type", "user_id", "created_at", "start_time", "stop_time"])
    return df

# Cell
def get_timestamps(ch):
    "Gets timestamps of all the samples and returns in ms"
    starts = pd.to_datetime(ch.start_time.astype("float"), unit="ms").values
    periods = ch.buffer_duration + 1 # Add +1 when using closed one way
    ends = starts + np.timedelta64(int((ch.buffer_duration[0] / ch.sampling_rate[0])*1000), "ms")
    ranges = np.array([pd.date_range(start=s, end=e, periods=p, closed='left') for s, e, p in zip(starts, ends, periods)]).flatten()
    return ranges

# Cell
def get_unique_timestamps(ch):
    data = ch.data.explode().values
    timestamps = get_timestamps(ch)
    assert len(data) == len(timestamps)

    df = pd.DataFrame(data = {'data':data,'timestamp':timestamps})
    if ch.iloc[0].buffer_interval >= ch.iloc[0].buffer_duration:
        return timestamps,np.array(df.index)

    # Drop duplicates with same data and time
    df_duplicate_data_time = df.drop_duplicates(subset=["data", "timestamp"], keep="first").sort_values(by=["timestamp"])

    # Drop duplicates with same time, keep last entry
    # **There may have different data values for the same timestamp
    df_duplicate_time = df_duplicate_data_time.drop_duplicates(subset=["timestamp"], keep="last").sort_values(by=["timestamp"])

    unique_timestamps = np.array(df_duplicate_time.index)

    erroneous_timestamps = len(df) - len(df_duplicate_time)

    if (erroneous_timestamps>0):
        print("Number of unique timestamps: ", len(timestamps))
        print(
            "Some timestamps may have different data values, this affected approximately",
            "%.2f" % (100 * erroneous_timestamps / len(df)),
            "% of the data",
        )

    # To do: Check if you have continous timestamps throughout
    # assert = not np.any(np.diff(timestamps[uniqueTimestamps], axis=0).astype(float)-3906250)

    return timestamps, unique_timestamps

# Cell
def print_info(ch):
    duration = int((ch.buffer_duration[0] / ch.sampling_rate[0]))  # in seconds
    print("Each buffer is", duration, "second(s) long")

    start_times_diff = 1e-9 * np.diff(pd.to_datetime(ch.start_time.astype("float"), unit="ms").values, axis=0).astype(float)
    interval = int((ch.buffer_interval[0] / ch.sampling_rate[0]))
    print("Each buffer is sampled every", interval, "second(s)")  # prints the most frequent element

    missed_samples = np.array(np.where((duration - start_times_diff) < 0)).flatten()
    print("The number of buffers skipped", len(missed_samples))

# Cell
def parse_channel(ch):
    data = ch.data.explode().values
    return pd.DataFrame(data=data)

# Cell
def biometric_to_df(biometric, ch_names):
    if biometric.empty:
        return None
    channels = [biometric.loc[biometric["channel"] == i] for i, ch in enumerate(ch_names)]
    for idx,ch in enumerate(channels[:-1]): assert (len(ch)==len(channels[idx+1])), "all channels should have the same length"

    ##Time consuming to find unique timestamps -> mostly relevant for Muse recordings

    timestamps, unique_timestamps = get_unique_timestamps(channels[0])
    df_cat = pd.concat([parse_channel(ch) for ch in channels], axis=1)
    df_cat.columns = ch_names
    df_cat["timestamp"] = timestamps
    df_unique = df_cat.iloc[unique_timestamps]
    df_biometric = pd.DataFrame(df_unique.set_index("timestamp"))
    return df_biometric

# Cell
muse_channels = ["TP9", "AF7", "AF8", "TP10"]
nexus_channels = ["TP9", "FT7", "AF7", "FP1", "FP2", "AF8", "FT8", "TP10"]
muse_sampling_rate = 256
nexus_sampling_rate = 250

# Cell
def query_eeg(metric_id):
    sql = f"""
    SELECT "channel","data","startTime","samplingRate","bufferDuration","bufferInterval"
    FROM "EEGSample"
    JOIN "EEG" on "EEG".id = "EEGSample"."eegId"
    WHERE "EEG"."metricId" = '{metric_id}'
    ORDER by "startTime","channel"
    """
    data = get_data(sql)
    df = pd.DataFrame(data, columns=["channel", "data", "start_time", "sampling_rate", "buffer_duration","buffer_interval"])
    return df

# Cell
def eeg_to_df(eeg, ch_names=nexus_channels):
    channels = [eeg.loc[eeg["channel"] == i] for i, ch in enumerate(ch_names)]
    for idx,ch in enumerate(channels[:-1]): assert (len(ch)==len(channels[idx+1])), "all channels should have the same length"
    print_info(channels[0])

    ##Time consuming to find unique timestamps -> mostly relevant for Muse recordings

    timestamps, unique_timestamps = get_unique_timestamps(channels[0])

    df_cat = pd.concat([parse_channel(ch) for ch in channels], axis=1)
    df_cat.columns = ch_names
    df_cat["time"] = timestamps
    # pd_cat['time_unixepoch'] =pd_cat['time'].astype('int64') * 0.000001
    df_unique = df_cat.iloc[unique_timestamps]
    df_eeg = df_unique.set_index("time")
    return df_eeg

# Cell
def df_to_mne(eeg_df, sfreq=nexus_sampling_rate, ch_names=nexus_channels):
    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types='eeg')
    raw = mne.io.RawArray(eeg_df[ch_names].T, info)
    montage = mne.channels.make_standard_montage('standard_1005')
    raw.set_montage(montage,match_case=False)
    return raw

# Cell
dpath = Path('./data')

# Cell
def fetch_eeg(metric_id):
    eeg = query_eeg(metric_id)
    ch_names = nexus_channels if len(np.unique(eeg['channel'])) == 8 else muse_channels

    #TODO: normalize voltage -> muse conversion
    eeg_df = eeg_to_df(eeg, ch_names=ch_names)
    return df_to_mne(eeg_df, sfreq=eeg.sampling_rate[0], ch_names=ch_names)

# Cell
def get_eeg(metric_id):
    if os.path.isdir(dpath) is False:
        os.mkdir(dpath)

    fpath = f"{dpath/metric_id}raw.fif"

    if os.path.exists(fpath):
        raw = mne.io.Raw(fpath)
    else:
        raw = fetch_eeg(metric_id)
        raw.save(fpath)
    return raw

# Cell
ppg_names = ['Ambient','Infrared','Red']

# Cell
def get_ppg(metric_id):
    sql = f"""
    SELECT "PPGSample".channel as "channel","data","startTime","samplingRate", "bufferDuration","bufferInterval"
    FROM "PPGSample"
    JOIN "PPG" on "PPG".id = "PPGSample"."ppgId"
    WHERE "PPG"."metricId" = '{metric_id}'
    ORDER by "startTime","channel"
    """
    data = get_data(sql)
    df = pd.DataFrame(data, columns=['channel','data','start_time','sampling_rate','buffer_duration','buffer_interval'])
    return df

# Cell
def get_ppg_timestamp(ch):
    starts = pd.to_datetime(ch.start_time.astype("float"), unit="ms").values
    periods = np.repeat(len(ch.iloc[0].samples), len(ch), axis=None)
    ends = starts + np.timedelta64(78125, "us")

    return np.array([pd.date_range(start=s, end=e, periods=p) for s, e, p in zip(starts, ends, periods)]).flatten()

# Cell
def parse_ppg(ch, name):
    data = {name: ch.sort_values(by=["timestamps"]).samples.explode().values}
    ppg = pd.DataFrame(data=data)
    ppg["timestamp"] = get_ppg_timestamp(ch)
    df_ppg = ppg.set_index("timestamp")
    # df_ppg.columns = ppg_name
    return df_ppg

# Cell
def ppg_to_df(metric_id):
    ppg = get_ppg(metric_id)

    if ppg.empty:
        return None
    ch_names = ['infrared'] if len(ppg['channel'].unique()) == 1 else ['ambient','infrared','red']
    channels = [ppg.loc[ppg["channel"] == i] for i, ch in enumerate(ch_names)]
    #timestamps = get_timestamps(channels[0])
    timestamps, unique_timestamps = get_unique_timestamps(channels[0])
    df_cat = pd.concat([parse_channel(ch) for ch in channels], axis=1)
    df_cat.columns = ch_names
    df_cat["timestamp"] = timestamps
    ppg_df = pd.DataFrame(df_cat.iloc[unique_timestamps].set_index("timestamp"))
    return  ppg_df

# Cell
def get_telemetry(metric_id):
    sql = f"""
    SELECT  "batteryLevel", "fuelGaugeVoltage", "temperature",timestamp
    FROM "Telemetry"
    WHERE "metricId" = '{metric_id}'
    ORDER BY timestamp Asc
    """
    data = get_data(sql)

    df = pd.DataFrame(data, columns=["battery", "voltage", "temperature", "timestamp"])
    df["time"] = pd.to_datetime(df.timestamp.astype("float"), unit="ms").values
    df = df.set_index("time")
    return df

# Cell
def get_task(metric_id):
    sql = f"""
    SELECT
    TRIAL.index,
    U.email,
    TASK.name, TASK.completed,  TRIAL.critical, TRIAL.stimulus, TRIAL.expected,
    RESP.result,RESP."presentedAt", RESP."respondedAt", RESP.response,
    TASK.comment

    FROM "User" as U
    JOIN "Task" as TASK on TASK."userId" = U.id
    JOIN "Trial" as TRIAL on TRIAL."taskId" = Task.id
    JOIN "TrialResponse" as RESP on RESP."trialId" = TRIAL.id
    WHERE TASK."metricId"='{metric_id}'
    """
    data = get_data(sql)
    return pd.DataFrame(
        data,
        columns=[
            "trial_index",
            "email",
            "name",
            "completed",
            "critical",
            "stimulus",
            "expected",
            "result",
            "presented_at",
            "responded_at",
            "response",
            "comment",
        ],
    )

# Cell
def print_task_info(task):
    task_name, task_level, task_trials = task.head(1)["name"][0].split("-")
    print("Task:", task_name, "\nLevel:", task_level, "\nTrials:", task_trials)
    Behave_result = (sum(task["result"]) / len(task["result"])) * 100
    print("Behavioral Accuracy: ", Behave_result, "%")
    response = task["responded_at"].astype("int64") - task["presented_at"].astype("int64")
    responseMean, responseStd = np.mean(response), np.std(response)
    print("Response time: ", responseMean, "+/-", responseStd, "ms")

# Cell
def sync_nback_task(data, task, event_dir):
    presented_at = pd.to_datetime(task.presented_at.astype("int64"), unit="ms").values
    responded_at = pd.to_datetime(task.responded_at.astype("int64"), unit="ms").values

    p_events = [[data.index.get_loc(t, method="nearest"), 0, event_dir["presented_at"]] for t in presented_at]
    r_events = [[data.index.get_loc(t, method="nearest"), event_dir["presented_at"], event_dir["responded_at"]] for t in responded_at]

    events = np.concatenate([p_events, r_events])
    events_df = pd.DataFrame(events, columns=["time_index", "prior_event", "event"]).set_index("time_index")

    return events_df.sort_index()

# Cell
def get_user_tasks(user_id):
    sql = f"""
    select "createdAt","completed",id,"name"
    from "Task"
    where "userId" ='{user_id}'
    Order by "createdAt" Desc
    """
    data = get_data(sql)
    return pd.DataFrame(data, columns=["created_at", "completed", "id", "name"])