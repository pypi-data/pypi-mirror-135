# ---------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# ---------------------------------------------------------
"""Functions used to generate Python script for customer usage."""
import inspect
import json
import logging
import os
import pickle
import shutil
import tempfile
from typing import Any, List, Optional, cast

from sklearn.pipeline import Pipeline

from azureml.core import Dataset, Run, Workspace

from azureml.automl.core import _codegen_utilities
from azureml.automl.core.shared._diagnostics.contract import Contract
from azureml.automl.core.shared.constants import Sample_Weights_Unsupported
from azureml.automl.runtime import data_cleaning
from azureml.automl.runtime.experiment_store import ExperimentStore
from azureml.automl.runtime.shared.model_wrappers import (
    ForecastingPipelineWrapper,
    PreFittedSoftVotingClassifier,
    PreFittedSoftVotingRegressor,
    StackEnsembleBase,
)
from azureml.train.automl import _constants_azureml

from . import utilities
from .constants import FunctionNames
from .ensemble_model_template import EnsembleModelTemplate, StackEnsembleModelTemplate
from .featurizer_template import AbstractFeaturizerTemplate
from .function import Function
from .model_template import AbstractModelTemplate, SingleModelTemplate
from .pipeline_step_template import PipelineStepTemplate
from .template_factory import featurizer_template_factory, preprocessor_template_factory, validation_template_factory
from .validation.validation_task import AbstractTask

logger = logging.getLogger(__name__)


def _get_setup_run(parent_run: Run) -> Run:
    setup_run_list = list(parent_run._client.run.get_runs_by_run_ids(run_ids=[f"{parent_run.id}_setup"]))
    # if this is a local run there will be no setup iteration
    if len(setup_run_list) == 0:
        setup_run = parent_run
    else:
        setup_run = setup_run_list[0]
    return setup_run


def split_dataset(X, y, weights, split_ratio, should_stratify):
    from sklearn.model_selection import train_test_split

    random_state = 42
    if should_stratify:
        stratify = y
    else:
        stratify = None

    if weights is not None:
        X_train, X_test, y_train, y_test, weights_train, weights_test = train_test_split(
            X, y, weights, stratify=stratify, test_size=split_ratio, random_state=random_state
        )
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, stratify=stratify, test_size=split_ratio, random_state=random_state
        )
        weights_train, weights_test = None, None

    return (X_train, y_train, weights_train), (X_test, y_test, weights_test)


def setup_instrumentation(run):
    parent_run = run.parent
    from azureml.telemetry import INSTRUMENTATION_KEY

    from azureml.automl.core.shared import log_server

    try:
        log_server.enable_telemetry(INSTRUMENTATION_KEY)
        log_server.set_verbosity(logging.INFO)
        log_server.update_custom_dimensions({"codegen_run_id": run.id, "parent_run_id": parent_run.id})
    except Exception:
        pass

    logger.info("Instrumentation setup complete.")


def generate_full_script(child_run: Run, pipeline: Optional[Any] = None) -> str:
    with _codegen_utilities.use_custom_repr():
        output = [
            "# ---------------------------------------------------------",
            "# Copyright (c) Microsoft Corporation. All rights reserved.",
            "# ---------------------------------------------------------",
            "# This file has been autogenerated by the Azure Automated Machine Learning SDK.",
            "\n",
            "import numpy",
            "import numpy as np",
            "import pickle",
            "import logging",
            "import argparse"
            "\n",
            "logger = logging.getLogger('azureml.train.automl.runtime.codegen_script')\n",
            "\n",
        ]
        # Add instrumentation code
        output.extend(_codegen_utilities.get_function_source(setup_instrumentation))

        # Add function for splitting dataset
        output.extend(_codegen_utilities.get_function_source(split_dataset))

        parent_run = child_run.parent
        properties = parent_run.properties

        training_dataset_id, validation_dataset_id = utilities.get_input_datasets(parent_run)

        settings_json = properties.get(_constants_azureml.Properties.AML_SETTINGS)
        settings_obj = json.loads(settings_json)

        task_type = cast(str, settings_obj.get("task_type"))
        label_column_name = cast(str, settings_obj.get("label_column_name"))
        weight_column_name = cast(Optional[str], settings_obj.get("weight_column_name"))
        metric_name = cast(str, settings_obj.get("primary_metric"))
        # test_size = cast(Optional[float], settings_obj.get("test_size"))
        validation_size = cast(Optional[float], settings_obj.get("validation_size"))
        n_cross_validations = cast(Optional[int], settings_obj.get("n_cross_validations"))
        y_min = cast(Optional[float], settings_obj.get("y_min"))
        y_max = cast(Optional[float], settings_obj.get("y_max"))
        is_timeseries = cast(bool, settings_obj.get("is_timeseries", False))

        validation_dataset_exists = validation_dataset_id is not None
        has_weights = weight_column_name is not None

        """
        # TODO: Figure out the best place to save this
        if not has_weights:
            try:
                expr_store = ExperimentStore.get_instance()
                sample_weights = expr_store.data._get_sw_train()
            except Exception:
                pass
        """

        if pipeline is None:
            tempdir = None
            try:
                tempdir = tempfile.mkdtemp()

                # Retrieve the preprocessor/algorithm sklearn pipeline
                child_run.download_file(_constants_azureml.MODEL_PATH, tempdir)
                pipeline_path = os.path.join(tempdir, os.path.basename(_constants_azureml.MODEL_PATH))
                with open(pipeline_path, "rb") as f:
                    pipeline = pickle.load(f)
            finally:
                if tempdir is not None:
                    shutil.rmtree(tempdir, ignore_errors=True)

        featurizer_template = featurizer_template_factory.select_template(pipeline, task_type)
        validation_task_template = validation_template_factory.select_template(
            task_type, metric_name, validation_dataset_exists, validation_size, n_cross_validations, y_min, y_max
        )

        output.extend(get_dataset_code(FunctionNames.GET_TRAIN_DATASET_FUNC_NAME))
        output.append("\n")
        if validation_dataset_exists:
            output.extend(get_dataset_code(FunctionNames.GET_VALID_DATASET_FUNC_NAME))
            output.append("\n")
        output.extend(
            get_prepare_data_code(validation_task_template, is_timeseries, label_column_name, weight_column_name)
        )
        output.append("\n")

        output.append(get_model_code(pipeline, featurizer_template, is_timeseries=is_timeseries))
        output.append("\n")
        output.extend(get_train_model_code(pipeline, has_weights))
        output.append("\n")
        output.extend(validation_task_template.get_scoring_function().generate_code())
        output.append("\n")
        output.extend(get_scriptrun_code(is_timeseries, validation_dataset_exists, validation_task_template))
        output.append("\n")
        output.append("if __name__ == '__main__':")
        output.append(get_dataset_parameter_code(training_dataset_id, validation_dataset_id))
        if validation_dataset_id is not None:
            output.append("    main(args.training_dataset_id, args.validation_dataset_id)")
        else:
            output.append("    main(args.training_dataset_id)")

        output_str = "\n".join(output)

        # Experimental auto formatting. We can't rely on this until black has a proper public API
        # See https://github.com/psf/black/issues/779
        # Maybe we can also add some automated cleanup of unused imports?
        # https://stackoverflow.com/questions/54325116/can-i-handle-imports-in-an-abstract-syntax-tree
        try:
            import black

            return black.format_str(output_str, mode=black.Mode())
        except Exception:
            return output_str


def get_dataset_parameter_code(training_dataset_id, validation_dataset_id):
    output = [
        "    parser = argparse.ArgumentParser()",
        f"parser.add_argument('--training_dataset_id', type=str, default=\'{training_dataset_id}\', \
help='Default training dataset id is populated from the parent run')"
    ]

    if validation_dataset_id is not None:
        output.append(
            f"parser.add_argument('--validation_dataset_id',  type=str, default=\'{validation_dataset_id}\', \
help='Default validation dataset id is populated from the parent run')")

    output.append("args = parser.parse_args()")
    output.append("")

    code_body = "\n".join(output)
    return _codegen_utilities.indent_multiline_string(code_body)


def get_dataset_code(dataset_func_name: str) -> List[str]:
    function = Function(dataset_func_name, "dataset_id")
    function.add_imports(Dataset, Run)
    function += [
        f'logger.info("Running {dataset_func_name}")',
        "ws = Run.get_context().experiment.workspace",
        "dataset = Dataset.get_by_id(workspace=ws, id=dataset_id)",
        "return dataset.to_pandas_dataframe()",
    ]

    return function.generate_code()


def get_prepare_data_code(
    validation_task_template: AbstractTask,
    is_timeseries: bool,
    label_column_name: str,
    weight_column_name: Optional[str] = None,
    test_size: Optional[float] = None,
) -> List[str]:
    # TODO: make stuff from data_transformation.py publicly visible here
    #  such as label encoding, dropping NaN, etc
    function = Function(FunctionNames.PREPARE_DATA_FUNC_NAME, "dataframe")
    function.add_imports(data_cleaning)
    function += [
        f'logger.info("Running {FunctionNames.PREPARE_DATA_FUNC_NAME}")',
        f"label_column_name = '{label_column_name}'",
        "",
        "# extract the features, target and sample weight arrays",
        "y = dataframe[label_column_name].values",
    ]

    if weight_column_name:
        function += [
            f"class_weights_column_name = '{weight_column_name}'",
            "X = dataframe.drop([label_column_name, class_weights_column_name], axis=1)",
            "sample_weights = dataframe[class_weights_column_name].values",
        ]
    else:
        function += ["X = dataframe.drop([label_column_name], axis=1)", "sample_weights = None"]

    # Remove the test set if needed
    # TODO: handle timeseries
    if not is_timeseries and test_size is not None and test_size > 0.0:
        function += [
            "",
            "# Split training data into train/test datasets and take only the train dataset",
            *validation_task_template.data_splitting_strategy.get_test_data_split_code(test_size),
        ]

    # TODO: Make this API public (#1277252)
    function += [
        "X, y, sample_weights = data_cleaning._remove_nan_rows_in_X_y(X, y, sample_weights,",
        f" is_timeseries={is_timeseries}, target_column=label_column_name)",
        "",
        "return X, y, sample_weights",
    ]

    return function.generate_code()


def get_model_code(
    sklearn_pipeline: Pipeline,
    featurizer_template: AbstractFeaturizerTemplate,
    is_timeseries: bool,
) -> str:
    output = featurizer_template.generate_featurizer_code()
    model = sklearn_pipeline.steps[-1][1]
    if isinstance(model, (PreFittedSoftVotingRegressor, PreFittedSoftVotingClassifier)):
        model_template = EnsembleModelTemplate(model)  # type: AbstractModelTemplate
        output += model_template.generate_model_code()
        pipeline_step_templates = [featurizer_template, model_template]
    elif isinstance(model, StackEnsembleBase):
        model_template = StackEnsembleModelTemplate(model)
        output += model_template.generate_model_code()
        pipeline_step_templates = [featurizer_template, model_template]
    else:
        preproc_template = preprocessor_template_factory.select_template(sklearn_pipeline)
        model_template = SingleModelTemplate(model)
        output += preproc_template.generate_preprocessor_code()
        output += model_template.generate_model_code()
        pipeline_step_templates = [featurizer_template, preproc_template, model_template]

    if is_timeseries:
        Contract.assert_type(sklearn_pipeline, "sklearn_pipeline", expected_types=ForecastingPipelineWrapper)
        sklearn_pipeline = cast(ForecastingPipelineWrapper, sklearn_pipeline)
        timeseries_stddev = cast(float, sklearn_pipeline.stddev)
        output.append(get_build_timeseries_model_pipeline_code(timeseries_stddev, featurizer_template))
    else:
        output.append(get_build_model_pipeline_code_from_templates(pipeline_step_templates))

    return "\n".join(output)


def get_build_model_pipeline_code_from_templates(pipeline_step_templates: List[PipelineStepTemplate]) -> str:
    output = [f"def {FunctionNames.BUILD_MODEL_FUNC_NAME}():"]

    imports = [_codegen_utilities.get_import(Pipeline)]
    output.extend(_codegen_utilities.generate_import_statements(imports))
    output.append("")

    output.append(f'logger.info("Running {FunctionNames.BUILD_MODEL_FUNC_NAME}")')
    output.append("pipeline = Pipeline(")
    output.append("    steps=[")

    for template in pipeline_step_templates:
        output.extend(_codegen_utilities.indent_lines(template.generate_pipeline_step(), 8))

    output.append("    ]")
    output.append(")")
    output.append("")
    output.append("return pipeline")

    code_body = "\n".join(output)
    return _codegen_utilities.indent_multiline_string(code_body)


def get_build_timeseries_model_pipeline_code(
    stddev: Optional[float], featurizer_template: AbstractFeaturizerTemplate
) -> str:
    output = [f"def {FunctionNames.BUILD_MODEL_FUNC_NAME}():"]

    imports = [
        _codegen_utilities.get_import(Pipeline),
        _codegen_utilities.get_import(ForecastingPipelineWrapper),
    ]
    output.extend(_codegen_utilities.generate_import_statements(imports))
    output.append("")

    output.append(f'logger.info("Running {FunctionNames.BUILD_MODEL_FUNC_NAME}")')
    output.append("pipeline = Pipeline(")
    output.append("    steps=[")
    output.extend(_codegen_utilities.indent_lines(featurizer_template.generate_pipeline_step(), 8))
    output.append(f"        ('model', {FunctionNames.MODEL_FUNC_NAME}())")
    output.append("    ]")
    output.append(")")
    output.append(f"forecast_pipeline_wrapper = ForecastingPipelineWrapper(pipeline, stddev={stddev})")
    output.append("")
    output.append("return forecast_pipeline_wrapper")

    code_body = "\n".join(output)
    return _codegen_utilities.indent_multiline_string(code_body)


def get_train_model_code(pipeline: Pipeline, use_weights: bool) -> List[str]:
    function = Function(FunctionNames.TRAIN_MODEL_FUNC_NAME, "X", "y", "sample_weights")
    function += [
        f'logger.info("Running {FunctionNames.TRAIN_MODEL_FUNC_NAME}")',
        f"model_pipeline = {FunctionNames.BUILD_MODEL_FUNC_NAME}()",
        "",
    ]
    """
    # Taken from _ml_engine/training/train.py
    # Add sample weights if model supports it
    # TODO: This doesn't work all the time. Re-enable when it does
    classifier_name = pipeline.steps[-1][0]
    if use_weights and classifier_name not in Sample_Weights_Unsupported:
        # pipeline expects kwargs to be formatted as stepname__arg.
        # The arg is then passed to fit of stepname
        function += f"model = model_pipeline.fit(" \
                    f"X, y, **{{f'{{model_pipeline.steps[-1][0]}}__sample_weight': sample_weights}})"
    else:
    """
    function += "model = model_pipeline.fit(X, y)"
    function += "return model"

    return function.generate_code()


def get_scriptrun_code(
    is_timeseries: bool, validation_dataset_exists: bool, validation_task_template: AbstractTask
) -> List[str]:
    if validation_dataset_exists:
        function = Function("main", "training_dataset_id=None", "validation_dataset_id=None")
    else:
        function = Function("main", "training_dataset_id=None")
    function.add_imports(Run)

    function += [
        "# The following code is for when running this code as part of an AzureML script run.",
        "run = Run.get_context()",
        f"{FunctionNames.SETUP_INSTRUMENTATION_FUNC_NAME}(run)",
        "",
        f"df = {FunctionNames.GET_TRAIN_DATASET_FUNC_NAME}(training_dataset_id)",
        f"X, y, sample_weights = {FunctionNames.PREPARE_DATA_FUNC_NAME}(df)",
    ]

    if is_timeseries:
        if validation_dataset_exists:
            function += [
                f"model = {FunctionNames.TRAIN_MODEL_FUNC_NAME}(X, y, sample_weights)",
                "",
                f"valid_df = {FunctionNames.GET_VALID_DATASET_FUNC_NAME}(validation_dataset_id)",
                f"X_valid, y_valid, sample_weights_valid = {FunctionNames.PREPARE_DATA_FUNC_NAME}(valid_df)",
                "y_pred = model.forecast(X_valid)",
                "",
            ]
        else:
            # TODO: this is predicting on the same data on the input (just to validate the model)
            # if validation dataset is not provided.
            #  Change this to use proper splitting.
            function += [
                f"model = {FunctionNames.TRAIN_MODEL_FUNC_NAME}(X, y, sample_weights)",
                "",
                "y_pred = model.forecast(X)",
                "",
            ]
    else:
        scoring_import_tuples, code_body = validation_task_template.validation_strategy.get_scoring_code()
        function.add_import_tuples(scoring_import_tuples)
        function += code_body

    # TODO: Emit code to log metrics

    function += [
        "with open('model.pkl', 'wb') as f:",
        "    pickle.dump(model, f)",
        "run.upload_file('outputs/model.pkl', 'model.pkl')",
    ]

    return function.generate_code()
