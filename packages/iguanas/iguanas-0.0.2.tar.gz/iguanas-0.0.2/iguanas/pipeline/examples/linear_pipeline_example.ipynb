{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4314dd6f",
   "metadata": {},
   "source": [
    "# Linear Pipeline Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3a33f2",
   "metadata": {},
   "source": [
    "A Linear Pipeline is a sequence of steps that are applied sequentially to a dataset. Each step should be an instantiated class with both `fit` and `transform` methods. The final step should be an instantiated class with both `fit` and `predict` methods.\n",
    "\n",
    "Pipelines will usually have the following high-level structure:\n",
    "\n",
    "1. Rule generation/optimisation step\n",
    "2. Rule processing steps\n",
    "3. Rule predictor step\n",
    "\n",
    "**Note that currently, it is not possible to have a rule optimisation step after a rule generation step (i.e. to optimise rules that have been generated), but this feature is being developed!**\n",
    "\n",
    "This example shows how to create a pipeline to perform a set of given steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be721b29",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddc510f",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb7115b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from iguanas.rule_generation import RuleGeneratorDT\n",
    "from iguanas.rule_selection import SimpleFilter, CorrelatedFilter\n",
    "from iguanas.metrics import FScore, Precision, JaccardSimilarity\n",
    "from iguanas.rbs import RBSOptimiser, RBSPipeline\n",
    "from iguanas.correlation_reduction import AgglomerativeClusteringReducer\n",
    "from iguanas.pipeline import LinearPipeline\n",
    "from iguanas.pipeline.class_accessor import ClassAccessor\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from category_encoders.one_hot import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6031c6",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28354c2",
   "metadata": {},
   "source": [
    "Let's read in the famous Titanic data set and split it into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "938ebd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../../examples/dummy_data/titanic.csv', index_col='PassengerId')\n",
    "target_col = 'Survived'\n",
    "cols_to_drop = ['Name', 'Ticket', 'Cabin']\n",
    "X = df.drop([target_col] + cols_to_drop, axis=1)\n",
    "y = df[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb3ee24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.33,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0499265",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae50c1c2",
   "metadata": {},
   "source": [
    "Let's apply the following simple steps to process the data:\n",
    "* One hot encode categorical variables (accounting for nulls)\n",
    "* Impute numeric features with -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fac7d3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlaidler/venvs/iguanas_os_dev/lib/python3.8/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    }
   ],
   "source": [
    "# OHE\n",
    "encoder = OneHotEncoder(\n",
    "    use_cat_names=True\n",
    ")\n",
    "X_train = encoder.fit_transform(X_train)\n",
    "X_test = encoder.transform(X_test)\n",
    "\n",
    "# Impute\n",
    "X_train.fillna(-1, inplace=True)\n",
    "X_test.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264a6921",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d043f0",
   "metadata": {},
   "source": [
    "## Set up pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d014bc20",
   "metadata": {},
   "source": [
    "Let's say that we want to apply the following processes as part of our pipeline:\n",
    "\n",
    "1. Rule generation step\n",
    "    * Use `RuleGeneratorDT` to generate rules using the processed data.\n",
    "2. Rule processing step\n",
    "    * Apply `SimpleFilter`, keeping rules with F1 score >= 0.1\n",
    "    * Apply `CorrelatedFilter`, removing rules with a Jaccard Similarity >= 0.9\n",
    "3. Rule predictor step\n",
    "    * Use the `RBSOptimiser` to optimise an `RBSPipeline` for F1 score. This will create a rule predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8ce09c",
   "metadata": {},
   "source": [
    "To create a pipeline to do this, let's first instantiate the relevant classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d6aa6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = FScore(beta=1)\n",
    "js = JaccardSimilarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6030498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule generation\n",
    "generator = RuleGeneratorDT(\n",
    "    metric=f1.fit,\n",
    "    n_total_conditions=4,\n",
    "    tree_ensemble=RandomForestClassifier(\n",
    "        n_estimators=10,\n",
    "        random_state=0\n",
    "    )\n",
    ")\n",
    "# Rule processing\n",
    "simple_filterer = SimpleFilter(\n",
    "    threshold=0.1, \n",
    "    operator='>=', \n",
    "    metric=f1.fit\n",
    ")\n",
    "corr_filterer = CorrelatedFilter(\n",
    "    correlation_reduction_class=AgglomerativeClusteringReducer(\n",
    "        threshold=0.9, \n",
    "        strategy='top_down', \n",
    "        similarity_function=js.fit, \n",
    "        metric=f1.fit\n",
    "    )\n",
    ")\n",
    "# Rule prediction\n",
    "rbs_pipeline = RBSPipeline(\n",
    "    config=[],\n",
    "    final_decision=0\n",
    ")\n",
    "rbs_optimiser = RBSOptimiser(\n",
    "    pipeline=rbs_pipeline,\n",
    "    metric=f1.fit, \n",
    "    pos_pred_rules=ClassAccessor(\n",
    "        class_tag='corr_filterer', \n",
    "        class_attribute='rules_to_keep'\n",
    "    ),\n",
    "    n_iter=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f915a7",
   "metadata": {},
   "source": [
    "**Note:** The argument passed to the `pos_pred_rules` parameter in the `RBSOptimiser` class is a `ClassAccessor` object. This takes the names of the rules that remain after the `CorrelatedFilter` has been applied and passes it to the `pos_pred_rules` parameter of the `RBSOptimiser` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1e338a",
   "metadata": {},
   "source": [
    "Now we can create the steps of our pipeline. Each step should be a tuple of two elements:\n",
    "\n",
    "1. The first element should be a string which refers to the step.\n",
    "2. The second element should be the instantiated class which runs at that step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf79e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    ('generator', generator),\n",
    "    ('simple_filterer', simple_filterer),\n",
    "    ('corr_filterer', corr_filterer),\n",
    "    ('rbs_optimiser', rbs_optimiser)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4fc4e4",
   "metadata": {},
   "source": [
    "Finally, we can instantiate our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a079750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = LinearPipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22183400",
   "metadata": {},
   "source": [
    "## Using the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f553b7",
   "metadata": {},
   "source": [
    "### `fit` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37c398b",
   "metadata": {},
   "source": [
    "By running the `fit` method, we sequentially run the `fit_transform` methods of each step in the pipeline, except for the last step, where the `fit` method is run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9041d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.fit(X_train, y_train, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c43da",
   "metadata": {},
   "source": [
    "#### Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9173994d",
   "metadata": {},
   "source": [
    "The `fit` method doesn't return anything. However, you can access the attributes of the fitted classes using the `get_params` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab23e54",
   "metadata": {},
   "source": [
    "### `fit_predict` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f2d716",
   "metadata": {},
   "source": [
    "By running the `fit_predict` method, we sequentially run the `fit_transform` methods of each step in the pipeline, except for the last step, where the `fit_predict` method is run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "363b1244",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = lp.fit_predict(X_train, y_train, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0adb3a",
   "metadata": {},
   "source": [
    "#### Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca32c00",
   "metadata": {},
   "source": [
    "The `fit_predict` method returns the prediction generated by class in the final step of the pipeline - in this case, the `RBSOptimiser`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7967465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId\n",
       "7      1\n",
       "719    1\n",
       "686    1\n",
       "74     1\n",
       "883    1\n",
       "      ..\n",
       "107    1\n",
       "271    1\n",
       "861    1\n",
       "436    1\n",
       "103    0\n",
       "Name: Stage=0, Decision=1, Length: 596, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6de8782",
   "metadata": {},
   "source": [
    "### `predict` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb64136f",
   "metadata": {},
   "source": [
    "By running the `predict` method, we sequentially run the `transform` methods of each step in the pipeline, except for the last step, where the `predict` method is run. Note that before using this method, you should first run either the `fit` or `fit_predict` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75773514",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = lp.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ce9eec",
   "metadata": {},
   "source": [
    "#### Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fe3abd",
   "metadata": {},
   "source": [
    "The `predict` method returns the prediction generated by class in the final step of the pipeline - in this case, the `RBSOptimiser`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "653aee49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId\n",
       "710    1\n",
       "440    1\n",
       "841    1\n",
       "721    1\n",
       "40     1\n",
       "      ..\n",
       "716    1\n",
       "526    1\n",
       "382    1\n",
       "141    1\n",
       "174    1\n",
       "Name: Stage=0, Decision=1, Length: 295, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6147013a",
   "metadata": {},
   "source": [
    "We can now calculate the F1 score of our pipeline using the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c2eaed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5651105651105651"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1.fit(y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecd0fe7",
   "metadata": {},
   "source": [
    "This approach is very powerful when optimising hyperparameters for the overall performance of a Rules-Based System - see the `BayesSearchCV` class in the `rule_selection` module for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b05e17",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a5a22224d030f6805b27da964f50b3905be89918ca593f843e32c3b2a80fa84"
  },
  "kernelspec": {
   "display_name": "iguanas_os_dev",
   "language": "python",
   "name": "iguanas_os_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
